{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/duonghung86/Injury-severity-classification/blob/main/VCA_2_1_MLP_earlystopping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bo-3JT-1cl14",
    "outputId": "c710009f-72c5-44ef-9e68-7b27f7c8d34e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 270.0 gigabytes of available RAM\n",
      "\n",
      "Current system-wide CPU utilization %:  25.0\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory,cpu_percent\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "print('Current system-wide CPU utilization %: ',cpu_percent())\n",
    "#Remove all warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ond2r97pZvoF",
    "outputId": "090a81b4-6b1f-46d8-abb5-725257820a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from collections import Counter\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler # Standardization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Machine learning algos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Imblearn\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler,BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler,NearMiss,EditedNearestNeighbours\n",
    "\n",
    "# Grid search\n",
    "from kerastuner.tuners import RandomSearch,Hyperband,BayesianOptimization\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import feature_column  # for data wrangling\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy,CategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Dense,Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy,CategoricalAccuracy\n",
    "from tensorflow_addons.metrics import CohenKappa,F1Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMFYXIMkFSak",
    "outputId": "d80430a7-b93b-421a-a2d9-3b1cbf3e5ba5"
   },
   "outputs": [],
   "source": [
    "url = 'https://github.com/duonghung86/Injury-severity-classification/blob/main/Prepared%20Texas%202019.zip?raw=true' \n",
    "data_path = tf.keras.utils.get_file(origin=url, fname=url.split('/')[-1].split('?')[0], extract=True)\n",
    "data_path = data_path.replace('%20',' ').replace('.zip','.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "UPLNCpWjZvoG",
    "outputId": "7d652183-53ce-4328-aada-ea393cc2efce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949856, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prsn_Injry_Sev</th>\n",
       "      <th>Prsn_Age</th>\n",
       "      <th>Prsn_Gndr</th>\n",
       "      <th>Wthr_Cond</th>\n",
       "      <th>Light_Cond</th>\n",
       "      <th>Surf_Cond</th>\n",
       "      <th>Veh_Body_Styl</th>\n",
       "      <th>Prsn_Rest</th>\n",
       "      <th>Prsn_Drg_Rslt</th>\n",
       "      <th>Harm_Evnt</th>\n",
       "      <th>Rural</th>\n",
       "      <th>Crash_Speed_Limit</th>\n",
       "      <th>Road_Algn</th>\n",
       "      <th>Veh_Mod_Year</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Crash_season</th>\n",
       "      <th>Part_of_day</th>\n",
       "      <th>Collsn_type</th>\n",
       "      <th>Collsn_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>MALE</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>DRY</td>\n",
       "      <td>PASSENGER CAR, 2-DOOR</td>\n",
       "      <td>SHOULDER &amp; LAP BELT</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>MOTOR VEHICLE IN TRANSPORT</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>STRAIGHT, LEVEL</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>SAME DIRECTION</td>\n",
       "      <td>ONE STRAIGHT-ONE LEFT TURN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>MALE</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>DRY</td>\n",
       "      <td>PASSENGER CAR, 2-DOOR</td>\n",
       "      <td>SHOULDER &amp; LAP BELT</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>MOTOR VEHICLE IN TRANSPORT</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>STRAIGHT, LEVEL</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>SAME DIRECTION</td>\n",
       "      <td>ONE STRAIGHT-ONE LEFT TURN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>DRY</td>\n",
       "      <td>PICKUP</td>\n",
       "      <td>SHOULDER &amp; LAP BELT</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>MOTOR VEHICLE IN TRANSPORT</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>CURVE, LEVEL</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>SAME DIRECTION</td>\n",
       "      <td>BOTH LEFT TURN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prsn_Injry_Sev  Prsn_Age Prsn_Gndr Wthr_Cond Light_Cond Surf_Cond  \\\n",
       "0               0        26      MALE     CLEAR   DAYLIGHT       DRY   \n",
       "1               0        52      MALE     CLEAR   DAYLIGHT       DRY   \n",
       "2               0        27    FEMALE     CLEAR   DAYLIGHT       DRY   \n",
       "\n",
       "           Veh_Body_Styl            Prsn_Rest   Prsn_Drg_Rslt  \\\n",
       "0  PASSENGER CAR, 2-DOOR  SHOULDER & LAP BELT  Not Applicable   \n",
       "1  PASSENGER CAR, 2-DOOR  SHOULDER & LAP BELT  Not Applicable   \n",
       "2                 PICKUP  SHOULDER & LAP BELT  Not Applicable   \n",
       "\n",
       "                    Harm_Evnt  Rural  Crash_Speed_Limit        Road_Algn  \\\n",
       "0  MOTOR VEHICLE IN TRANSPORT      0                 -1  STRAIGHT, LEVEL   \n",
       "1  MOTOR VEHICLE IN TRANSPORT      0                 -1  STRAIGHT, LEVEL   \n",
       "2  MOTOR VEHICLE IN TRANSPORT      1                 -1     CURVE, LEVEL   \n",
       "\n",
       "   Veh_Mod_Year  Weekend  Crash_season  Part_of_day     Collsn_type  \\\n",
       "0            33        1             3            3  SAME DIRECTION   \n",
       "1            19        1             3            3  SAME DIRECTION   \n",
       "2            16        1             3            4  SAME DIRECTION   \n",
       "\n",
       "                  Collsn_name  \n",
       "0  ONE STRAIGHT-ONE LEFT TURN  \n",
       "1  ONE STRAIGHT-ONE LEFT TURN  \n",
       "2              BOTH LEFT TURN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1CDPbDPYZvoH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(949856, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's just use 80% of the total dataset\n",
    "#df, _ = train_test_split(df, test_size=0.9,stratify = df['Prsn_Injry_Sev'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEKbv3R9ZvoH",
    "outputId": "93f16610-1110-4ffe-9c66-f4dea88ae652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All target values:\n",
      "0    792558\n",
      "1    102409\n",
      "2     45242\n",
      "3      7951\n",
      "4      1696\n",
      "Name: Prsn_Injry_Sev, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df['Prsn_Injry_Sev']\n",
    "print('All target values:')\n",
    "print(y.value_counts())\n",
    "X = df.drop(columns=['Prsn_Injry_Sev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUDtEBDsZvoI",
    "outputId": "b311b931-acd8-453a-95f9-b54c0b572d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables are  ['Prsn_Age', 'Rural', 'Crash_Speed_Limit', 'Veh_Mod_Year', 'Weekend', 'Crash_season', 'Part_of_day']\n",
      "Categorical variables that have at most 5 categories are  ['Prsn_Gndr', 'Prsn_Drg_Rslt', 'Collsn_type']\n",
      "Categorical variables that have more than 5 categories are  ['Wthr_Cond', 'Light_Cond', 'Surf_Cond', 'Veh_Body_Styl', 'Prsn_Rest', 'Harm_Evnt', 'Road_Algn', 'Collsn_name']\n",
      "WARNING:tensorflow:From /project/cacds/apps/easybuild/software/Anaconda3/5.0.1/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /project/cacds/apps/easybuild/software/Anaconda3/5.0.1/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "New shape of the input data set: (949856, 59)\n"
     ]
    }
   ],
   "source": [
    "# %% Data wrangling -------------\n",
    "# Classify variable type\n",
    "emb_vars, ind_vars, num_vars = [], [], []\n",
    "for var in X.columns:\n",
    "    if X[var].dtypes == 'O':\n",
    "        if len(X[var].unique()) > 5:\n",
    "            emb_vars.append(var)\n",
    "        else:\n",
    "            ind_vars.append(var)\n",
    "    else:\n",
    "        num_vars.append(var)\n",
    "print('Numerical variables are ', num_vars)\n",
    "print('Categorical variables that have at most 5 categories are ', ind_vars)\n",
    "print('Categorical variables that have more than 5 categories are ', emb_vars)\n",
    "\n",
    "# Create feature columns\n",
    "feature_columns = []\n",
    "# numeric cols\n",
    "for header in num_vars:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "# bucketized cols\n",
    "# age = feature_column.numeric_column('Prsn_Age')\n",
    "# age_buckets = feature_column.bucketized_column(age, boundaries=[16, 22, 35, 55, 65])\n",
    "# feature_columns.append(age_buckets)\n",
    "# indicator_columns\n",
    "for col_name in ind_vars:\n",
    "    categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        col_name, X[col_name].unique())\n",
    "    indicator_column = feature_column.indicator_column(categorical_column)\n",
    "    feature_columns.append(indicator_column)\n",
    "# embedding columns\n",
    "for col_name in emb_vars:\n",
    "    emb_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        col_name, X[col_name].unique())\n",
    "    col_embedding = feature_column.embedding_column(emb_column, dimension=5)\n",
    "    feature_columns.append(col_embedding)\n",
    "\n",
    "# Convert all setup into new dataset\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "X = feature_layer(dict(X)).numpy()\n",
    "print('New shape of the input data set:',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EiMyeEfZvoJ",
    "outputId": "a73f13c7-932e-49e5-bb80-6c9d506ad140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (607907, 59)\n",
      "Validation features shape: (151977, 59)\n",
      "Test features shape: (189972, 59)\n"
     ]
    }
   ],
   "source": [
    "# %% Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=48)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=48)\n",
    "\n",
    "print('Training features shape:', X_train.shape)\n",
    "print('Validation features shape:', X_val.shape)\n",
    "print('Test features shape:', X_test.shape)\n",
    "\n",
    "# %% standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ds5G6Pjhes9"
   },
   "source": [
    "# ALL mini functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Metrics\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix,accuracy_score\n",
    "\n",
    "# %% Function to compare the prediction and true labels\n",
    "def get_accs(label, pred_proba, tr_time=0,index=None):\n",
    "    prediction = pred_proba.argmax(axis=1)\n",
    "    cm = confusion_matrix(label, prediction)\n",
    "    length = cm.shape[0]\n",
    "    num_cases = len(label)\n",
    "    # global accuracy\n",
    "    glb_acc = np.trace(cm) / len(label)\n",
    "    ind_accs = cm / np.sum(cm, axis=1)[:, np.newaxis]\n",
    "    accs = [ind_accs[i, i] for i in range(length)]\n",
    "    cols = ['Class {}'.format(i) for i in range(length)]\n",
    "    # Global accuracy\n",
    "    accs.append(glb_acc)\n",
    "    # AUC\n",
    "    accs.append(roc_auc_score(label, pred_proba,multi_class='ovr'))\n",
    "    # G-mean\n",
    "    accs.append(geometric_mean_score(label, prediction, correction=0.001))\n",
    "    # Average perf\n",
    "    accs.append(np.mean(accs[-3:]))\n",
    "    # Training time\n",
    "    accs.append(np.round(tr_time,3))\n",
    "    cols = cols + ['Accuracy','AUC','G-mean','Avg_Pfm','Training Time']\n",
    "\n",
    "    out = np.array(accs).reshape(1, len(accs))\n",
    "    return pd.DataFrame(out, columns=cols,index=[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML with class weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff6Y5pBDhsXm"
   },
   "source": [
    "# MLP functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmzSlMdDZvoK",
    "outputId": "3e186bc5-a34c-42fc-90a7-98f73ea8d376"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.23969347661941065,\n",
       " 1: 1.8550434079431195,\n",
       " 2: 4.199122746425364,\n",
       " 3: 23.891019846728238,\n",
       " 4: 111.95340699815839}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add weights\n",
    "weights = len(y_train) / (5 * np.bincount(y_train))\n",
    "cls_wgt = dict(zip(np.arange(5), weights))\n",
    "cls_wgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w5IRiyFuZvoK"
   },
   "outputs": [],
   "source": [
    "def early_stops(metric_name):\n",
    "    es = EarlyStopping(monitor='val_'+ metric_name,\n",
    "                   verbose=1, patience=10, mode='max',\n",
    "                   restore_best_weights=True)\n",
    "    return es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant\n",
    "EPOCH = 50\n",
    "BATCH_SIZE = 2048\n",
    "VERBOSE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [tf.keras.metrics.AUC(name='auc'),\n",
    "           tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall')\n",
    "          ]\n",
    "def create_mlp():\n",
    "    MLP = Sequential([Dense(10,\n",
    "                           activation='relu',\n",
    "                           input_dim=X_train.shape[1],\n",
    "                           ),\n",
    "                      Dropout(0.5),\n",
    "                      Dense(5, activation='softmax')])\n",
    "    MLP.compile(optimizer='adam',\n",
    "                loss=CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=METRICS\n",
    "               )\n",
    "    return MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Resampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under sampling ...\n",
      "Counter({0: 65541, 1: 65541, 2: 28954, 3: 5089, 4: 1086})\n",
      "over sampling #2 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Counter({0: 65541, 1: 65541, 2: 65541, 3: 65541, 4: 65541}),\n",
       " 68.83484697341919)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict = Counter(y_train)\n",
    "\n",
    "start = time.time()\n",
    "res = RandomUnderSampler(random_state = 54, sampling_strategy={0: y_dict[1]})\n",
    "print('under sampling ...')\n",
    "X_res, y_res = res.fit_resample(X_train, y_train)\n",
    "\n",
    "res = SMOTE(random_state = 34,sampling_strategy='not majority')\n",
    "print(Counter(y_res))\n",
    "print('over sampling #2 ...')\n",
    "X_res, y_res = res.fit_resample(X_res, y_res)\n",
    "end = time.time()\n",
    "res_time = end-start\n",
    "Counter(y_res),res_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Hybrid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00027: early stopping\n",
      "          Accuracy       AUC    G-mean   Avg_Pfm  Training Time\n",
      "MLP-HS#1  0.510380  0.681984  0.347929  0.513431         23.261\n",
      "MLP-HS#2  0.491667  0.688674  0.354012  0.511451         37.616\n",
      "MLP-HS#3  0.488914  0.704585  0.370056  0.521185         40.780\n"
     ]
    }
   ],
   "source": [
    "rsts = pd.DataFrame()\n",
    "for i in range(3):\n",
    "    model = create_mlp()\n",
    "    start = time.time()\n",
    "    monitor = model.fit(X_res,pd.get_dummies(y_res).values,\n",
    "                        callbacks=[early_stops('accuracy')],\n",
    "                        validation_data=(X_val,pd.get_dummies(y_val).values),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        verbose=VERBOSE, epochs=EPOCH\n",
    "                       )\n",
    "    end = time.time()\n",
    "    # use the model to make predictions with the test data\n",
    "    Y_pred = model.predict(X_test)\n",
    "    rsts = rsts.append(get_accs(y_test.values,Y_pred,end-start,'MLP-HS#'+str(i+1)))\n",
    "print(rsts.iloc[:,5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up grid search\n",
    "\n",
    "We will investigates the following parameters:\n",
    "\n",
    "- Initial weights\n",
    "- Activation function\n",
    "- Number of nodes\n",
    "- Dropout rate\n",
    "- Early Stop\n",
    "- Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    hp_units = hp.Int('units', min_value=5, max_value=15, step=5)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "    hp_dos = hp.Float('dropouts',min_value=0.2, max_value=0.5, step=0.1)\n",
    "    hp_acts = hp.Choice('activation', values = ['relu','sigmoid','tanh','selu'])\n",
    "    keins = ['uniform', 'lecun_uniform', 'normal', 'zeros', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "    hp_keins = hp.Choice('kernel_ini', values = keins) \n",
    "    model = Sequential([Dense(hp_units,\n",
    "                           activation=hp_acts,\n",
    "                           input_dim=X_train.shape[1],\n",
    "                            kernel_initializer= hp_keins \n",
    "                           ),\n",
    "                      Dropout(hp_dos),\n",
    "                      Dense(5, activation='softmax')])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss=CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=METRICS\n",
    "               )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metr = ['auc','accuracy','precision','recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 5\n",
    "FACTOR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.5 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Tuning time is 115.69\n",
      "{'units': 10, 'learning_rate': 0.001, 'dropouts': 0.30000000000000004, 'activation': 'sigmoid', 'kernel_ini': 'he_uniform', 'tuner/epochs': 5, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Tuning time is 118.25\n",
      "{'units': 15, 'learning_rate': 0.01, 'dropouts': 0.30000000000000004, 'activation': 'selu', 'kernel_ini': 'lecun_uniform', 'tuner/epochs': 3, 'tuner/initial_epoch': 0, 'tuner/bracket': 1, 'tuner/round': 0}\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Tuning time is 120.59\n",
      "{'units': 5, 'learning_rate': 0.01, 'dropouts': 0.5000000000000001, 'activation': 'tanh', 'kernel_ini': 'glorot_uniform', 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 2, 'tuner/round': 0}\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Tuning time is 121.26\n",
      "{'units': 10, 'learning_rate': 0.01, 'dropouts': 0.5000000000000001, 'activation': 'tanh', 'kernel_ini': 'lecun_uniform', 'tuner/epochs': 5, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
      "                              0              0               0              0\n",
      "units                        10             15               5             10\n",
      "learning_rate             0.001           0.01            0.01           0.01\n",
      "dropouts                    0.3            0.3             0.5            0.5\n",
      "activation              sigmoid           selu            tanh           tanh\n",
      "kernel_ini           he_uniform  lecun_uniform  glorot_uniform  lecun_uniform\n",
      "tuner/epochs                  5              3               2              5\n",
      "tuner/initial_epoch           0              0               0              0\n",
      "tuner/bracket                 0              1               2              0\n",
      "tuner/round                   0              0               0              0\n",
      "Tuning_time             115.686        118.248         120.595        121.256\n",
      "                 Accuracy       AUC    G-mean   Avg_Pfm  Training Time\n",
      "MLP-HS#1         0.510380  0.681984  0.347929  0.513431         23.261\n",
      "MLP-HS#2         0.491667  0.688674  0.354012  0.511451         37.616\n",
      "MLP-HS#3         0.488914  0.704585  0.370056  0.521185         40.780\n",
      "MLP-auc-1        0.596862  0.681671  0.095840  0.458124        115.686\n",
      "MLP-auc-2        0.565731  0.634357  0.247320  0.482469        115.686\n",
      "MLP-accuracy-1   0.499963  0.707956  0.387071  0.531663        118.248\n",
      "MLP-accuracy-2   0.457631  0.702376  0.381038  0.513681        118.248\n",
      "MLP-precision-1  0.551766  0.692755  0.335366  0.526629        120.595\n",
      "MLP-precision-2  0.495099  0.686316  0.351018  0.510811        120.595\n",
      "MLP-recall-1     0.495362  0.703266  0.350349  0.516326        121.256\n",
      "MLP-recall-2     0.489741  0.694070  0.351571  0.511794        121.256\n"
     ]
    }
   ],
   "source": [
    "bps = pd.DataFrame()\n",
    "for obj in metr:\n",
    "    start = time.time()\n",
    "    if obj =='accuracy':\n",
    "        goal = obj\n",
    "    else:\n",
    "        goal = kt.Objective('val_'+obj, direction=\"max\")\n",
    "    tuner = Hyperband(build_model,\n",
    "                     objective = goal, \n",
    "                     max_epochs = MAX_EPOCHS,\n",
    "                     factor = FACTOR,\n",
    "                     directory = 'my_dir',\n",
    "                     project_name = 'val_'+ obj+'_'+time.ctime())\n",
    "    tuner.search(X_res, pd.get_dummies(y_res).values,\n",
    "                 epochs=MAX_EPOCHS,batch_size=2048,\n",
    "                 verbose=0,\n",
    "                 callbacks=[early_stops(obj)],\n",
    "                 validation_data=(X_val, pd.get_dummies(y_val).values))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Tuning time is %.2f' % (end-start))\n",
    "    print(tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values)\n",
    "    bp = pd.Series(tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values,name=obj)\n",
    "    #bp = bp.append(pd.Series(end-start,index=['Tuning_time']))\n",
    "    bps = pd.concat((bps,bp),axis=1)\n",
    "    models = tuner.get_best_models(num_models=FACTOR)\n",
    "    for i in range(FACTOR):\n",
    "        Y_pred = models[i].predict(X_test)\n",
    "        rsts = rsts.append(get_accs(y_test.values,Y_pred,end-start,'MLP-'+obj+'-'+str(i+1)))\n",
    "        rsts.to_csv('VCA_Tuning2.csv')\n",
    "print(bps)\n",
    "print(rsts.iloc[:,5:])\n",
    "bps.to_csv('VCA_Tuning2_bps.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "VCA_2.1_MLP_earlystopping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
